import type { Context } from '@mariozechner/pi-ai'
import { completeSimple } from '@mariozechner/pi-ai'
import type { DocumentPrompt } from '../prompt.js'
import type { LlmTokenUsage } from '../types.js'
import { normalizeTokenUsage } from '../usage.js'
import { createUnsupportedFunctionalityError } from '../errors.js'
import type { OpenAiClientConfig } from './types.js'
import { resolveOpenAiModel } from './models.js'
import { bytesToBase64 } from './shared.js'

export type OpenAiClientConfigInput = {
  apiKeys: {
    openaiApiKey: string | null
    openrouterApiKey: string | null
  }
  forceOpenRouter?: boolean
  openaiBaseUrlOverride?: string | null
  forceChatCompletions?: boolean
}

export function resolveOpenAiClientConfig({
  apiKeys,
  forceOpenRouter,
  openaiBaseUrlOverride,
  forceChatCompletions,
}: OpenAiClientConfigInput): OpenAiClientConfig {
  const baseUrlRaw =
    openaiBaseUrlOverride ??
    (typeof process !== 'undefined' ? process.env.OPENAI_BASE_URL : undefined)
  const baseUrl =
    typeof baseUrlRaw === 'string' && baseUrlRaw.trim().length > 0 ? baseUrlRaw.trim() : null
  const isOpenRouterViaBaseUrl = baseUrl ? /openrouter\.ai/i.test(baseUrl) : false
  const hasOpenRouterKey = apiKeys.openrouterApiKey != null
  const hasOpenAiKey = apiKeys.openaiApiKey != null
  const isOpenRouter =
    Boolean(forceOpenRouter) ||
    isOpenRouterViaBaseUrl ||
    (hasOpenRouterKey && !baseUrl && !hasOpenAiKey)

  const apiKey = isOpenRouter
    ? (apiKeys.openrouterApiKey ?? apiKeys.openaiApiKey)
    : apiKeys.openaiApiKey
  if (!apiKey) {
    throw new Error(
      isOpenRouter
        ? 'Missing OPENROUTER_API_KEY (or OPENAI_API_KEY) for OpenRouter'
        : 'Missing OPENAI_API_KEY for openai/... model'
    )
  }

  const baseURL = forceOpenRouter
    ? 'https://openrouter.ai/api/v1'
    : (baseUrl ?? (isOpenRouter ? 'https://openrouter.ai/api/v1' : undefined))

  const isCustomBaseURL = (() => {
    if (!baseURL) return false
    try {
      const url = new URL(baseURL)
      return url.host !== 'api.openai.com' && url.host !== 'openrouter.ai'
    } catch {
      return false
    }
  })()

  const useChatCompletions = Boolean(forceChatCompletions) || isOpenRouter || isCustomBaseURL
  return {
    apiKey,
    baseURL: baseURL ?? undefined,
    useChatCompletions,
    isOpenRouter,
  }
}

function normalizeOpenAiUsage(raw: unknown): LlmTokenUsage | null {
  if (!raw || typeof raw !== 'object') return null
  const usage = raw as { input_tokens?: unknown; output_tokens?: unknown; total_tokens?: unknown }
  const promptTokens =
    typeof usage.input_tokens === 'number' && Number.isFinite(usage.input_tokens)
      ? usage.input_tokens
      : null
  const completionTokens =
    typeof usage.output_tokens === 'number' && Number.isFinite(usage.output_tokens)
      ? usage.output_tokens
      : null
  const totalTokens =
    typeof usage.total_tokens === 'number' && Number.isFinite(usage.total_tokens)
      ? usage.total_tokens
      : typeof promptTokens === 'number' && typeof completionTokens === 'number'
        ? promptTokens + completionTokens
        : null
  if (promptTokens === null && completionTokens === null && totalTokens === null) return null
  return { promptTokens, completionTokens, totalTokens }
}

function resolveOpenAiResponsesUrl(baseUrl: string): URL {
  const url = new URL(baseUrl)
  const path = url.pathname.replace(/\/$/, '')
  if (/\/responses$/.test(path)) {
    url.pathname = path
    return url
  }
  if (/\/v1$/.test(path)) {
    url.pathname = `${path}/responses`
    return url
  }
  url.pathname = `${path}/v1/responses`
  return url
}

function extractOpenAiResponseText(payload: {
  output_text?: unknown
  output?: Array<{ content?: Array<{ text?: string }> }>
}): string {
  if (typeof payload.output_text === 'string') return payload.output_text.trim()
  const output = Array.isArray(payload.output) ? payload.output : []
  const text = output
    .flatMap((item) => (Array.isArray(item.content) ? item.content : []))
    .map((block) => (typeof block.text === 'string' ? block.text : ''))
    .join('')
    .trim()
  return text
}

export async function completeOpenAiText({
  modelId,
  openaiConfig,
  context,
  temperature,
  maxOutputTokens,
  signal,
}: {
  modelId: string
  openaiConfig: OpenAiClientConfig
  context: Context
  temperature?: number
  maxOutputTokens?: number
  signal: AbortSignal
}): Promise<{ text: string; usage: LlmTokenUsage | null }> {
  const model = resolveOpenAiModel({ modelId, context, openaiConfig })
  const result = await completeSimple(model, context, {
    ...(typeof temperature === 'number' ? { temperature } : {}),
    ...(typeof maxOutputTokens === 'number' ? { maxTokens: maxOutputTokens } : {}),
    apiKey: openaiConfig.apiKey,
    signal,
  })
  const text = result.content
    .filter((c) => c.type === 'text')
    .map((c) => c.text)
    .join('')
    .trim()
  if (!text) throw new Error(`LLM returned an empty summary (model openai/${modelId}).`)
  return { text, usage: normalizeTokenUsage(result.usage) }
}

export async function completeOpenAiDocument({
  modelId,
  openaiConfig,
  prompt,
  maxOutputTokens,
  temperature,
  timeoutMs,
  fetchImpl,
}: {
  modelId: string
  openaiConfig: OpenAiClientConfig
  prompt: DocumentPrompt
  maxOutputTokens?: number
  temperature?: number
  timeoutMs: number
  fetchImpl: typeof fetch
}): Promise<{ text: string; usage: LlmTokenUsage | null }> {
  if (openaiConfig.isOpenRouter) {
    throw createUnsupportedFunctionalityError(
      'OpenRouter does not support PDF attachments for openai/... models'
    )
  }
  const baseUrl = openaiConfig.baseURL ?? 'https://api.openai.com/v1'
  const host = new URL(baseUrl).host
  if (host !== 'api.openai.com') {
    throw createUnsupportedFunctionalityError(`Document attachments require api.openai.com; got ${host}`)
  }

  const url = resolveOpenAiResponsesUrl(baseUrl)
  const controller = new AbortController()
  const timeout = setTimeout(() => controller.abort(), timeoutMs)
  const filename = prompt.document.filename?.trim() || 'document.pdf'
  const payload = {
    model: modelId,
    input: [
      {
        role: 'user',
        content: [
          {
            type: 'input_file',
            filename,
            file_data: bytesToBase64(prompt.document.bytes),
          },
          { type: 'input_text', text: prompt.text },
        ],
      },
    ],
    ...(typeof maxOutputTokens === 'number' ? { max_output_tokens: maxOutputTokens } : {}),
    ...(typeof temperature === 'number' ? { temperature } : {}),
  }

  try {
    const response = await fetchImpl(url, {
      method: 'POST',
      headers: {
        'content-type': 'application/json',
        authorization: `Bearer ${openaiConfig.apiKey}`,
      },
      body: JSON.stringify(payload),
      signal: controller.signal,
    })

    const bodyText = await response.text()
    if (!response.ok) {
      const error = new Error(`OpenAI API error (${response.status}).`)
      ;(error as { statusCode?: number }).statusCode = response.status
      ;(error as { responseBody?: string }).responseBody = bodyText
      throw error
    }

    const data = JSON.parse(bodyText) as {
      output_text?: unknown
      output?: Array<{ content?: Array<{ text?: string }> }>
      usage?: unknown
    }
    const text = extractOpenAiResponseText(data)
    if (!text) {
      throw new Error(`LLM returned an empty summary (model openai/${modelId}).`)
    }
    return { text, usage: normalizeOpenAiUsage(data.usage) }
  } finally {
    clearTimeout(timeout)
  }
}
